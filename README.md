### This repository contains the simulation code, datasets and plotting code for the article "Optimising play for learning risky behaviour" by Dharanish Rajendra & Chaitanya S. Gokhale.

## Overview

This repository is organised into two folders:

1. `SimulationCode` contains the code used for running simulations and regenerating the dataset required for creating the figures
2. `FigureSources` contains the scripts used for plotting the figures 
3. `Datasets` contains the datasets generated by the codes in SimulationCodes and are used in the scripts in FigureSources to create the figures

## Installing Julia

All of the code in this repository is written in the `Julia` programming language. 
To run the simulations and replicate the code, one needs to install `Julia v1.10.2` and all the packages required. 
As the code here was written for v1.10.2 of the language, we recommend that this version is used to ensure perfect reproducibility.

1. Follow the instructions here: https://julialang.org/install/. This will install `juliaup`, the version manager for Julia and also the latest stable version of the Julia programming language. 
2. Install v1.10.2 of the language `juliaup add 1.10.2`. 
3. To make this the default version, run the command `juliaup default 1.10.2`. 
4. Otherwise, every time Julia is run, one would have to add `julia +1.10.2` in order to launch the desired version. 

## Installing required packages

The required Julia environment with the specific versions of the necessary packages can be recreated with the `Project.toml` and `Manifest.toml` files. 
The process to do this is as follows:
1. Open a terminal in the directory of this repository
2. Launch a Julia REPL with: `julia --project=.`
3. Install all the packages with: `] instantiate`
4. Hit `backspace` to exit the package manager. Now you are ready to run the scripts in this repository.

## SimulationCode

This folder contains the code to perform the simulations to generate the dataset required for creating the figures.
These files can be run, by first starting a Julia REPL as Step 1-2 above and running the following: `include("SimulationCode/<name of file>")`, where `<name of file>` is replaced by the name of the code file.


1. `adult_environment_optimal_performance.jl` calculates the optimal policy for a range of parameter values for ρ (dangerous prey abundance), ϕ (capture probability) and Ei (energy cost of injury) using the Dynamic Programming algorithm Value Iteration. 
It then simulates an agent using this policy in order to calculate the optimal performance (average rewards per episode). 
This generates the dataset file `adult_environment_optimal_performance.arrow`.
This dataset is required to run the next simulation code. 
Run this before the next file.
2. `protected_environment_learning_simulations.jl` performs the two-stage learning simulation and calculates the relevant metrics of adult performance and adult relearning time for a range of environmental parameter values and developmental times. 
This generates the dataset file `protected_environment_learning.arrow` which is required for creating Figures 3 – 7, A2 and A3.
3. `optimality_of_RL.jl` calculates the optimal policy which a reinforcement agent converges to and the time (in number of steps) taken to converge to the optimal policy for a range of ρ and ϕ values. 
This generates the dataset `optimality_of_RL.arrow` which is required to create Figure A1. 
4. `RLUtilities.jl` contains common Reinforcement Learning code used by all the other simulation scripts.

Each of these codes are very computationally expensive and were run on a High Performance Computing Cluster with hundreds of CPU cores running for several days. The codes can be run as-is on a cluster managed by SLURM and will scale to any resource configuration (multiple cores on a single node or distributed across nodes). 
To run on a personal computer and have it finish in a reasonable amount of time, the parameters of the simulations (within each script file) can be modified in the following ways.
- Reduce the number of experiments for each parameter combination by modifying the variable `number_experiments`
- Reduce the number of parameters of `ρ` and `ϕ`, by changing the step length. That is, instead of `ρ=0.01:0.01:0.99`, having `ρ=0.1:0.1:0.9` will reduce the number of parameters by a factor of 10 and make the figure more blocky.
- Reduce the number of values of `developmental_time` in `protected_environment_learning_simulations.jl`

Upon running any code, it will display / output a dynamic progress bar which will give an estimate of how much time is remaining.

The full dataset generated by these simulation codes is available in the folder `Datasets`.

## FigureSources

This folder contains all the datasets generated by the simulation codes above.
The codes will display and save a pdf of the Figure corresponding to their names. 
To create the figures, first start a Julia REPL as above and run the following: `include("FigureSources/Figure_<number>.jl")`.
Here `<number>` is to be replaced by the number of the Figure required (except Figure 1).

- Figure_1.svg is the editable vector graphics file that was used to create Figure 1, the illustration of the model.
- Figure_2.jl performs dynamic programming calculations to determine the optimal policy for a range of parameter values. No dataset is required for this file.
- Figure_3.jl – Figure_7.jl, Figure_A2.jl and Figure_A3.jl use the dataset `protected_environment_learning.arrow`
- Figure_A1.jl uses the dataset `optimality_of_RL.arrow`.

## Datasets

This folder contains the two datasets `protected_environment_learning.arrow` and `optimality_of_RL.arrow`. These are tables stored in the [Arrow](https://arrow.apache.org/) format. 